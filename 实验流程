进行中文文本摘要工作，需要完成以下工作内容：
|+  所做工作的目的：为了生成摘要从而减少索引证据的时间
|+  数据集中的事实部分的预处理
|+  证据和事实之间的信息关系，假设需要数据来证明
|+  证据全集作为事实去和事实进行比对


|+  实现baseline方法
    |@  各个方法中使用的公用接口，文书预处理以及内容编码
        {
            |+  产生字典
                {
                    ##  字典的产生方式有多种思路：
                    |&  直接对所有的数据集进行分词，对分词数据进行统计，然后去除频率低的词，作为词典数据
                    |&  参考之前进行的最大匹配长度，得到的一些文书中的特有表达段，并将其作为分词的自定义词典
                        {
                            |+  使用最大匹配的算法的目的是为了得到文书写作中特定的词级别的使用模式，
                            |+  比如，'经审理查明'这种短句子就是这种广义上的"词"，这种在文档中的固定搭配被当作一个特定的词来处理应但是比被额外分词处理来的好的
                            |+  但是使用最长匹配的方式还是会出现一些问题，这额问题主要是算法设计上的问题

                        }
                    |&  在tf-idf值过高的一些词往往表示了每一个案件中的特有的信息，比如所人名或地名，使用这些信息
                }
            |+  根据字典生成id序列向量
            |+  在字典中没有id的词分为两种情况，一种是诸如数字之类的停用此，一种是确实没有id
                [在处理某一个案件的时候，如果说出现了没有id的单词，那么这个单词是否就应该被抛弃呢，这个单词应但是在本文档中具有相当的特别性
                 所以应当给予异地过的重视，只是还没找到方法]
            |+  由于处理的是文本摘要的问题，会产生两段文本向量，分别是原文本以及长文本的代表向量
        }
    |&  ABS(attention-based summarization)
        {
           |+   使用预处理的接口，生成文本向量
           |+   encoder将文本向量读入并训练，encoder所读入的数据要包括x和y，由于需要生成的字符编码作为y，这里就涉及到了y的一些问题
           {
                ##  在一开始进行encoder的时候，显然应但是没有y的值的，这个时候应当输入什么样的y值呢，
                |&  输入的y值都是0，即作为unk输入
                |&  输入y之后进行补0，即在embedding级上
           }
           |+   使用encoder的过程中已经使用了一次attention，这个attention是使用一个矩阵先后和文章内容以及yc相乘得到的
           |+   yc的获取分为两种情况，
           {
                |&  直接从正确的摘要当中提取
                |&  从文本生成的摘要当中提取
                |+  从当前的位置向前提取C个y作为上下文选择的内容
           }
           |+   建立nnlm模型，将x和yc输入，得到每一个单词的概率分布
           |+   从得到的概率分布中计算真实值的负对数似然值，并相加作为模型的损失函数
           |+   使用mini-batch 随机梯度下降算法进行训练
        }
    |&  SESS(selection encoding sentence summarization)
        {

        }
