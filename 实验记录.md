
试验记录
======================
### 11/6
+ 在使用最原始的模型进行训练的时候，产生的效果很不理想，大部分的产出数据都是unk标签，而且训练的过程在大概第1个epoch就已经收敛，这种情况一般都是模型的参数无法满足吧
+ 另外，查看数据集，可以发现数据集中的数据存在一定的问题，有的案件的事实非常短，没有任何的事实和证据之间的摘要联系，这种数据显然极大的影响了模型的训练，我决定对数据集进行清洗
+ 今天下午还尝试训练了ABS生成模型，但是由于数据集以及预处理过程中的代码存在bug导致在运行一万步左右的时候程序总是会报错，这个bug今天已经解决了，但是还没有开始真正的训练
+ 今天还改变了GEFG模型之中的损失函数，改用交叉熵作为损失函数，还没有进行训练来验证
+ 另外一种可以作为损失函数的思路是使用词向量余弦距离作为loss值，将模型最后一步输出为一个和词典的词向量大小相同的向量，将他和正确单词的余弦距离作为loss函数，这两种方式应该都可以避免unk标签的出现

总结起来下一次实验的改进点为：

> + 数据集的清洗，删去使用频次特别低的证据以及事实描述
> + 信息映射方式增加字映射方式，扩大词典借以减少unk标签的影响
> + 在输出的内容中放弃unk标签，即训练数据中的fact不再包含unk
> + 改变损失函数，使用交叉熵作为损失函数
> + 使用静态的词向量，增加余弦相似度的方式作为损失函数
> + 生成器使用的信息向量使用对证据文本进行attention操作之后的向量取代lstm最后一步的状态作为信息。

还有一些框架结构上的改进，已经完成，每一个模型都使用基本一致的训练框架，区别在于模型的定义以及数据提供的格式


### 11/7

 

        
        