
试验记录
======================
### 11/6
+ 在使用最原始的模型进行训练的时候，产生的效果很不理想，大部分的产出数据都是unk标签，而且训练的过程在大概第1个epoch就已经收敛，这种情况一般都是模型的参数无法满足吧
+ 另外，查看数据集，可以发现数据集中的数据存在一定的问题，有的案件的事实非常短，没有任何的事实和证据之间的摘要联系，这种数据显然极大的影响了模型的训练，我决定对数据集进行清洗
+ 今天下午还尝试训练了ABS生成模型，但是由于数据集以及预处理过程中的代码存在bug导致在运行一万步左右的时候程序总是会报错，这个bug今天已经解决了，但是还没有开始真正的训练
+ 今天还改变了GEFG模型之中的损失函数，改用交叉熵作为损失函数，还没有进行训练来验证
+ 另外一种可以作为损失函数的思路是使用词向量余弦距离作为loss值，将模型最后一步输出为一个和词典的词向量大小相同的向量，将他和正确单词的余弦距离作为loss函数，这两种方式应该都可以避免unk标签的出现

总结起来下一次实验的改进点为：

> + 数据集的清洗，删去使用频次特别低的证据以及事实描述
> + 信息映射方式增加字映射方式，扩大词典借以减少unk标签的影响
> + 在输出的内容中放弃unk标签，即训练数据中的fact不再包含unk
> + 改变损失函数，使用交叉熵作为损失函数
> + 使用静态的词向量，增加余弦相似度的方式作为损失函数
> + 生成器使用的信息向量使用对证据文本进行attention操作之后的向量取代lstm最后一步的状态作为信息。

还有一些框架结构上的改进，已经完成，每一个模型都使用基本一致的训练框架，区别在于模型的定义以及数据提供的格式



### 11/14

训练出了ABS模型，并且进行了验证，存在一些问题：
1. ABS模型的生成文本通顺度不够，很多句子可读性很差。
2. 生成器不会自动的结束生成，只会不断根据上下文产生下面的文字。
3. 生成器没有产生标点符号，造成读起来的通顺度有问题
4. 有一些摘要信息有错误，和原始文本有出入。

从ROUGE结果上看，训练出的ABS模型是可以达到原论文中的指标


|Model | ROUGE-1 | ROUGE-2 | ROUGE-L |
|---|:---:|:---:|:---:|
|原论文 | 26.55 | 7.06 | 22.05 |
|复现代码|38.24|13.45|21.84

改进思路：
1. 在训练模型中添加结束符号以及逗号。
2. 使用词典代替字典。
3. 修改新模型的生成部分，借用ABS的生成部分。

### 11/21

需要对GEFG进行修正，修改的主要方向是：

1. gate值的计算目前使用的方式是使用证据文本的lstm最终状态编码和context向量进行attention，这种方式不能良好的反应所有的文本局部信息。需要加入更多局部信息。
2. 进行生成的时候使用的也是来自证据文本的终状态，无法捕捉文本内部的关系，可以使用attention方式来产生状态

### 12/17

对GEFG进行了修正，但是训练结果依然不尽人意，从结果上看可能由于梯度的爆炸和梯度消失产生的，查找相关的解决方案，可行的方案
是改变激活函数，或者进行梯度剪裁。